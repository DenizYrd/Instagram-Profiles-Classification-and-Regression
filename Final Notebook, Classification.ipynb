{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9ddf5f47-4b28-4925-9471-af28b9c031b8",
      "metadata": {
        "id": "9ddf5f47-4b28-4925-9471-af28b9c031b8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.read_json('training-dataset.jsonl', lines=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "20a9a79c",
      "metadata": {
        "collapsed": true,
        "id": "20a9a79c"
      },
      "outputs": [],
      "source": [
        "df = pd.json_normalize(dataframe['profile']) #creates dataframe for the profile\n",
        "df_posts_ = pd.json_normalize(dataframe['posts']) #creates dataframe for the posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "461dd5f8",
      "metadata": {
        "collapsed": true,
        "id": "461dd5f8"
      },
      "outputs": [],
      "source": [
        "#unpacks the objects in df_posts_\n",
        "def unpack_nested_column(df, column_name):\n",
        "    unpacked_df = pd.json_normalize(df[column_name])\n",
        "    unpacked_df.columns = [f\"{column_name}_{subcol}\" for subcol in unpacked_df.columns]\n",
        "    return unpacked_df\n",
        "\n",
        "# Iterate over all columns, unpack nested ones, and store in a list\n",
        "unpacked_dfs = []\n",
        "for col in df_posts_.columns:\n",
        "    if isinstance(df_posts_[col].iloc[0], dict):\n",
        "        unpacked_df = unpack_nested_column(df_posts_, col)\n",
        "        df_posts_ = df_posts_.drop(columns=[col]).join(unpacked_df)\n",
        "    else:\n",
        "        unpacked_dfs.append(df_posts_[[col]])\n",
        "\n",
        "# Concatenate the remaining DataFrames (non-nested columns)\n",
        "if unpacked_dfs:\n",
        "    non_nested_df = pd.concat(unpacked_dfs, axis=1)\n",
        "    result_df = pd.concat([non_nested_df, df_posts_], axis=1)\n",
        "else:\n",
        "    result_df = df_posts_\n",
        "\n",
        "df_posts_ = result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "503a4265",
      "metadata": {
        "collapsed": true,
        "id": "503a4265"
      },
      "outputs": [],
      "source": [
        "df_posts_ = df_posts_.drop(columns=[col for col in df_posts_.columns if 'media_url' in col])\n",
        "df_posts_ = df_posts_.drop(columns=[col for col in df_posts_.columns if 'id' in col])\n",
        "df_posts_ = df_posts_.drop(columns=[col for col in df_posts_.columns if 'timestamp' in col])\n",
        "df_posts_ = df_posts_.drop(columns=[col for col in df_posts_.columns if 'comments_count' in col])\n",
        "df_posts_ = df_posts_.drop(columns=[col for col in df_posts_.columns if 'like_count' in col])\n",
        "df_posts_ = df_posts_.drop(columns=[col for col in df_posts_.columns if 'media_type' in col])\n",
        "\n",
        "# Combine all caption columns into one\n",
        "df_posts_['combined'] = df_posts_.filter(like='caption').apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Drop the original caption columns\n",
        "df_posts_ = df_posts_.drop(columns=[col for col in df_posts_.columns if '_caption' in col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ed07161c",
      "metadata": {
        "id": "ed07161c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "df = df.drop('is_private', axis=1)\n",
        "df = df.drop('profile_pic_url', axis=1)\n",
        "df = df.drop('profile_picture_base64', axis=1)\n",
        "df = df.drop('business_phone_number', axis=1)\n",
        "df = df.drop('eimu_id', axis=1)\n",
        "df = df.drop('fbid', axis=1)\n",
        "df = df.drop('fb_profile_biolink', axis=1)\n",
        "df = df.drop('id', axis=1)\n",
        "df = df.drop('is_professional_account', axis=1)\n",
        "df = df.drop('ai_agent_type', axis=1)\n",
        "df = df.drop('restricted_by_viewer', axis=1)\n",
        "df = df.drop('business_email', axis=1)\n",
        "df = df.drop('is_regulated_c18', axis=1)\n",
        "df = df.drop('entities', axis=1)\n",
        "df = df.drop('overall_category_name', axis=1)\n",
        "df = df.drop('post_count', axis=1)\n",
        "df = df.drop('bio_links', axis=1)\n",
        "df = df.drop('business_address_json', axis=1)\n",
        "df = df.drop('category_enum', axis=1)\n",
        "df.drop(['should_show_category', 'should_show_public_contacts' ] , axis=1, inplace=True)\n",
        "df.drop(['hide_like_and_view_counts','highlight_reel_count', 'business_contact_method'], axis=1, inplace=True)\n",
        "\n",
        "uniform_columns = []\n",
        "\n",
        "for column in df.columns:\n",
        "    if df[column].nunique() == 1:\n",
        "        uniform_columns.append(column)\n",
        "for column in uniform_columns:\n",
        "    df = df.drop(column, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4ddf788d",
      "metadata": {
        "id": "4ddf788d"
      },
      "outputs": [],
      "source": [
        "# Create a new boolean column based on 'external_url'\n",
        "df['external_url_bool'] = df['external_url'].apply(lambda x: x is not None)\n",
        "\n",
        "# Drop the original 'external_url' column\n",
        "df = df.drop('external_url', axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "03930aa6",
      "metadata": {
        "id": "03930aa6"
      },
      "outputs": [],
      "source": [
        "# List of column names to concatenate\n",
        "columns_to_combine = ['full_name','biography', 'category_name', 'business_category_name']  # Replace with your actual column names\n",
        "\n",
        "# Concatenate the string columns into a new column 'combined_text'\n",
        "df['combined_text'] = df[columns_to_combine].fillna('').agg(' '.join, axis=1)\n",
        "df = df.drop('full_name', axis=1)\n",
        "df = df.drop('biography', axis=1)\n",
        "df = df.drop('category_name', axis=1)\n",
        "df = df.drop('business_category_name', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "dee60587",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee60587",
        "outputId": "c5581414-3561-4d30-d4a4-faa4f35bac1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "# Download Turkish stopwords\n",
        "nltk.download('stopwords')\n",
        "turkish_stopwords = stopwords.words('turkish')\n",
        "english_stopwords = stopwords.words('english')\n",
        "for i in range(len(english_stopwords)):\n",
        "    english_stopwords[i] = english_stopwords[i].lower()\n",
        "for i in range(len(turkish_stopwords)):\n",
        "    turkish_stopwords[i] = turkish_stopwords[i].casefold()\n",
        "\n",
        "stopwords = turkish_stopwords + english_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1958445b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1958445b",
        "outputId": "21aceaf6-161d-4d79-db8d-f0397664a125"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1368: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Creating columns using TF-IDF for most used 8500 words in combined text in profile dataframe\n",
        "\n",
        "\n",
        "# Define your custom vocabulary\n",
        "custom_vocab = ['Politician', 'Public & Government Service', 'Şehir', 'Government organization' , 'Belediye', 'Başkanlığı', 'tr', 'Government organization']  # Replace with your custom words\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_text(text: str):\n",
        "    # Lowercase Turkish text using casefold\n",
        "    text = text.casefold()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "# Apply preprocessing to the 'combined_text' column\n",
        "df['combined_text'] = df['combined_text'].apply(preprocess_text)\n",
        "\n",
        "# Initialize TfidfVectorizer with Turkish stopwords and max features\n",
        "vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=8500)\n",
        "\n",
        "# Fit and transform the 'combined_text' column\n",
        "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
        "\n",
        "# Get feature names (the dynamically selected top words)\n",
        "dynamic_features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Combine dynamic features with custom vocabulary, ensuring no duplicates\n",
        "final_vocab = list(set(dynamic_features).union(custom_vocab))\n",
        "\n",
        "# Reinitialize TfidfVectorizer with the combined vocabulary\n",
        "final_vectorizer = TfidfVectorizer(vocabulary=final_vocab)\n",
        "\n",
        "# Transform the text again with the combined vocabulary\n",
        "final_tfidf_matrix = final_vectorizer.fit_transform(df['combined_text'])\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "tfidf_df = pd.DataFrame(final_tfidf_matrix.toarray(), columns=final_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Concatenate the original DataFrame with the TF-IDF DataFrame\n",
        "df = pd.concat([df, tfidf_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "92ff6df4",
      "metadata": {
        "collapsed": true,
        "id": "92ff6df4"
      },
      "outputs": [],
      "source": [
        "#Creating columns using TF-IDF for most used 8500 words in combined text in posts dataframe\n",
        "\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_text(text: str):\n",
        "    # Lowercase Turkish text using casefold\n",
        "    text = text.casefold()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "# Apply preprocessing to the 'combined_text' column\n",
        "df_posts_['combined'] = df_posts_['combined'].apply(preprocess_text)\n",
        "\n",
        "# Initialize TfidfVectorizer with Turkish stopwords and max features\n",
        "vectorizer_posts = TfidfVectorizer(stop_words=stopwords, max_features=8500)\n",
        "\n",
        "# Fit and transform the 'combined_text' column\n",
        "tfidf_matrix_posts = vectorizer_posts.fit_transform(df_posts_['combined'])\n",
        "\n",
        "# Get feature names (the dynamically selected top words)\n",
        "dynamic_features_posts = vectorizer_posts.get_feature_names_out()\n",
        "\n",
        "# Combine dynamic features with custom vocabulary, ensuring no duplicates\n",
        "final_vocab_posts = list(set(dynamic_features_posts))\n",
        "\n",
        "# Reinitialize TfidfVectorizer with the combined vocabulary\n",
        "final_vectorizer_posts = TfidfVectorizer(vocabulary=final_vocab_posts)\n",
        "\n",
        "# Transform the text again with the combined vocabulary\n",
        "final_tfidf_matrix_posts = final_vectorizer_posts.fit_transform(df_posts_['combined'])\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "tfidf_df_posts = pd.DataFrame(final_tfidf_matrix_posts.toarray(), columns=final_vectorizer_posts.get_feature_names_out())\n",
        "\n",
        "# Concatenate the original DataFrame with the TF-IDF DataFrame\n",
        "df = pd.concat([df, tfidf_df_posts], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "f446fdaa",
      "metadata": {
        "id": "f446fdaa"
      },
      "outputs": [],
      "source": [
        "#Adding training data\n",
        "\n",
        "train_classification_df = pd.read_csv(\"train-classification.csv\",)\n",
        "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
        "\n",
        "# Unifying labels\n",
        "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
        "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ab928277",
      "metadata": {
        "id": "ab928277"
      },
      "outputs": [],
      "source": [
        "# Adding training data\n",
        "# Add a row at the beginning of the dataframe to avoid detecting a feature as a column name\n",
        "test_classification_df = pd.read_csv(\"test-classification-round3.dat\",header=None)\n",
        "test_classification_df.columns = ['user_id']\n",
        "# Unifying labels\n",
        "username2_category_test = test_classification_df['user_id'].apply(str.lower).str.strip().tolist()\n",
        "username2_category_test = [uname.lower().strip() for uname in username2_category_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "dbfa76ea",
      "metadata": {
        "id": "dbfa76ea"
      },
      "outputs": [],
      "source": [
        "# Create a boolean mask for train and test sets\n",
        "is_train = df['username'].isin(username2_category.keys())\n",
        "is_test = df['username'].apply(str.lower).str.strip().isin(username2_category_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "88c28be4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88c28be4",
        "outputId": "b5d0f377-e57e-4d8f-e72e-93e9a06981ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set shape: (2741, 17016)\n",
            "Test set shape: (1000, 17016)\n"
          ]
        }
      ],
      "source": [
        "# Split the dataframe into train and test sets\n",
        "train_df = df[is_train].reset_index(drop=True)\n",
        "test_df = df[is_test].reset_index(drop=True)\n",
        "\n",
        "# Check if all usernames in username2_category_test are in test_df\n",
        "for i in username2_category_test:\n",
        "    if i not in test_df['username'].values:\n",
        "        print(i)\n",
        "        username2_category_test.remove(i)\n",
        "\n",
        "# Display the shapes of the resulting dataframes\n",
        "print(\"Train set shape:\", train_df.shape)\n",
        "print(\"Test set shape:\", test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "69bfb20c",
      "metadata": {
        "id": "69bfb20c"
      },
      "outputs": [],
      "source": [
        "train_usernames = train_df['username']\n",
        "\n",
        "y = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
        "\n",
        "X = train_df.drop(['username','combined_text'], axis=1)\n",
        "\n",
        "X_Test = test_df.drop(['username','combined_text'], axis=1)\n",
        "\n",
        "X.columns = X.columns.astype(str)\n",
        "X_Test.columns = X_Test.columns.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "b6b215eb",
      "metadata": {
        "id": "b6b215eb"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import BorderlineSMOTE                                            \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Convert non-numeric features to numeric\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X = X.fillna(0)\n",
        "\n",
        "X_Test = pd.get_dummies(X_Test, drop_first=True)\n",
        "X_Test = X_Test.fillna(0)\n",
        "\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Balance classes using SMOTE\n",
        "smote = BorderlineSMOTE(random_state=42, k_neighbors=1, sampling_strategy='not minority', m_neighbors=11, kind='borderline-1')\n",
        "X_resampled, y_resampled= smote.fit_resample(X, y_encoded)\n",
        "\n",
        "# Scale features for better optimization\n",
        "scaler = StandardScaler()\n",
        "X_resampled = scaler.fit_transform(X_resampled)\n",
        "X_Test = scaler.fit_transform(X_Test)\n",
        "\n",
        "# Train SGDClassifier\n",
        "clf = SGDClassifier(random_state=42, max_iter=3000, tol=1e-3, class_weight='balanced', loss='log_loss', n_jobs=-1)\n",
        "clf.fit(X_resampled, y_resampled) #used full training data\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_deniz_test = clf.predict(X_Test)\n",
        "y_pred_decoded_test = label_encoder.inverse_transform(y_pred_deniz_test)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "978aaa3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "978aaa3e",
        "outputId": "f4d3034f-63a8-471f-9aae-bab60510d0e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of categories changed to 'Health and Lifestyle': 41\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "# Create a dictionary with usernames as keys and predicted labels as values\n",
        "username_label_dict = dict(zip(test_df['username'], y_pred_decoded_test))\n",
        "\n",
        "# Convert the dictionary to a JSON object\n",
        "json_object = json.dumps(username_label_dict, indent=4)\n",
        "\n",
        "predictions_dict = username_label_dict\n",
        "\n",
        "# Convert the dictionary into a DataFrame for easier processing\n",
        "predictions = pd.DataFrame(list(predictions_dict.items()), columns=[\"username\", \"category\"])\n",
        "\n",
        "# Count initial values for potential changes(belediye)\n",
        "initial_count = predictions.loc[\n",
        "    predictions['username'].str.contains('belediye', case=False, na=False) &\n",
        "    (predictions['category'] != 'Health and Lifestyle')\n",
        "].shape[0]\n",
        "\n",
        "# Update the category for usernames containing 'belediye'\n",
        "predictions.loc[predictions['username'].str.contains('belediye', case=False, na=False), 'category'] = 'Health and Lifestyle'\n",
        "\n",
        "# Count initial values for potential changes (bld)\n",
        "initial_count_1 = predictions.loc[\n",
        "    predictions['username'].str.contains('bld', case=False, na=False) &\n",
        "    (predictions['category'] != 'Health and Lifestyle')\n",
        "].shape[0]\n",
        "\n",
        "# Update the category for usernames containing 'bld'\n",
        "predictions.loc[predictions['username'].str.contains('bld', case=False, na=False), 'category'] = 'Health and Lifestyle'\n",
        "\n",
        "# Count initial values for potential changes(bel.tr)\n",
        "initial_count_2 = predictions.loc[\n",
        "    predictions['username'].str.contains('bel.tr', case=False, na=False) &\n",
        "    (predictions['category'] != 'Health and Lifestyle')\n",
        "].shape[0]\n",
        "\n",
        "# Update the category for usernames containing 'bel.tr'\n",
        "predictions.loc[predictions['username'].str.contains('bel.tr', case=False, na=False), 'category'] = 'Health and Lifestyle'\n",
        "\n",
        "# Update the category for usernames containing 'bel'\n",
        "predictions.loc[predictions['username'].str.endswith('bel'), 'category'] = 'Health and Lifestyle'\n",
        "\n",
        "predictions['category'] = predictions['category'].str.title()\n",
        "\n",
        "# Replace 'And' with 'and' in the 'category' column\n",
        "predictions['category'] = predictions['category'].str.replace('And', 'and')\n",
        "\n",
        "# Convert the DataFrame back into a dictionary\n",
        "updated_predictions_dict = dict(zip(predictions['username'], predictions['category']))\n",
        "\n",
        "# Save the updated dictionary to a JSON file\n",
        "with open(\"predictionround3.json\", \"w\") as f:\n",
        "    json.dump(updated_predictions_dict, f, indent=4)\n",
        "\n",
        "# Print the count of changes\n",
        "print(f\"Number of categories changed to 'Health and Lifestyle': {initial_count+initial_count_1+initial_count_2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91781876",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final reordering, I believe this may be unnecessary. It's here just in case.\n",
        "import json\n",
        "\n",
        "# Read the .dat file to get the list of usernames\n",
        "with open('test-classification-round3.dat', 'r') as file:\n",
        "    usernames = [line.strip() for line in file]\n",
        "\n",
        "# Read the JSON file to get the current mapping of usernames to predictions\n",
        "with open('predictionround3.json', 'r') as file:\n",
        "    predictions = json.load(file)\n",
        "\n",
        "# Create a new ordered dictionary\n",
        "ordered_predictions = {username: predictions[username] for username in usernames if username in predictions}\n",
        "\n",
        "# Check for any missing usernames or predictions\n",
        "missing_from_dat = set(predictions.keys()) - set(usernames)\n",
        "missing_from_json = set(usernames) - set(predictions.keys())\n",
        "\n",
        "if missing_from_dat:\n",
        "    print(\"Warning: The following usernames are in the JSON file but not in the .dat file:\")\n",
        "    print(missing_from_dat)\n",
        "\n",
        "if missing_from_json:\n",
        "    print(\"Warning: The following usernames are in the .dat file but not in the JSON file:\")\n",
        "    print(missing_from_json)\n",
        "\n",
        "# Write the new ordered JSON object back to a file\n",
        "with open('prediction-classification-round3.json', 'w') as file:\n",
        "    json.dump(ordered_predictions, file, indent=4)\n",
        "\n",
        "print(\"Reordering complete. Please check the warnings above, if any.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
