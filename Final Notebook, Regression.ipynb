{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a208b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\serha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Download Turkish stopwords\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')\n",
    "\n",
    "dataframe = pd.read_json('training-dataset.jsonl', lines=True)\n",
    "df = pd.json_normalize(dataframe['profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ddf5f47-4b28-4925-9471-af28b9c031b8",
   "metadata": {
    "id": "9ddf5f47-4b28-4925-9471-af28b9c031b8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = df.drop('is_private', axis=1)\n",
    "df = df.drop('profile_pic_url', axis=1)\n",
    "df = df.drop('profile_picture_base64', axis=1)\n",
    "df = df.drop('business_phone_number', axis=1)\n",
    "df = df.drop('eimu_id', axis=1)\n",
    "df = df.drop('fbid', axis=1)\n",
    "df = df.drop('fb_profile_biolink', axis=1)\n",
    "df = df.drop('id', axis=1)\n",
    "df = df.drop('is_professional_account', axis=1)\n",
    "df = df.drop('ai_agent_type', axis=1)\n",
    "df = df.drop('restricted_by_viewer', axis=1)\n",
    "df = df.drop('business_email', axis=1)\n",
    "df = df.drop('is_regulated_c18', axis=1)\n",
    "df = df.drop('entities', axis=1)\n",
    "df = df.drop('overall_category_name', axis=1)\n",
    "\n",
    "\n",
    "uniform_columns = []\n",
    "\n",
    "for column in df.columns:\n",
    "    if df[column].nunique() == 1:\n",
    "        uniform_columns.append(column)\n",
    "for column in uniform_columns:\n",
    "    df = df.drop(column, axis=1)\n",
    "\n",
    "\n",
    "df['external_url_bool'] = df['external_url'].apply(lambda x: x is not None)\n",
    "df = df.drop('external_url', axis=1)\n",
    "\n",
    "df['contact_call'] = df['business_contact_method'] == 'CALL'\n",
    "df['contact_unknown'] = df['business_contact_method'] == 'UNKNOWN'\n",
    "df['contact_text'] = df['business_contact_method'] == 'TEXT'\n",
    "df = df.drop('business_contact_method', axis=1)\n",
    "df = df.drop('bio_links', axis=1)\n",
    "df = df.drop('business_address_json', axis=1) \n",
    "df = df.drop('category_enum', axis=1) \n",
    "columns_to_combine = ['full_name','biography', 'category_name', 'business_category_name']  \n",
    "df['combined_text'] = df[columns_to_combine].fillna('').agg(' '.join, axis=1)\n",
    "df = df.drop('full_name', axis=1)\n",
    "df = df.drop('biography', axis=1)\n",
    "df = df.drop('category_name', axis=1)\n",
    "df = df.drop('business_category_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1958445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BurayÄ± yapmana gerek yok\n",
    "# Define your custom vocabulary\n",
    "custom_vocab = ['#example', 'ðŸ™‚', 'Ã¶zel', 'kelime']  # Replace with your custom words\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text: str):\n",
    "    # Lowercase Turkish text using casefold\n",
    "    text = text.casefold()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'combined_text' column\n",
    "df['combined_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Initialize TfidfVectorizer with Turkish stopwords and max features\n",
    "vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=2000)\n",
    "\n",
    "# Fit and transform the 'combined_text' column\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# Get feature names (the dynamically selected top words)\n",
    "dynamic_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Combine dynamic features with custom vocabulary, ensuring no duplicates\n",
    "final_vocab = list(set(dynamic_features).union(custom_vocab))\n",
    "\n",
    "# Reinitialize TfidfVectorizer with the combined vocabulary\n",
    "final_vectorizer = TfidfVectorizer(vocabulary=final_vocab)\n",
    "\n",
    "# Transform the text again with the combined vocabulary\n",
    "final_tfidf_matrix = final_vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(final_tfidf_matrix.toarray(), columns=final_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n",
    "df = df.drop('combined_text', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2ecc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8228962818003914\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.86      0.94      0.90       102\n",
      "       entertainment       0.64      0.75      0.69       102\n",
      "             fashion       0.83      0.84      0.83       102\n",
      "                food       0.74      0.72      0.73       103\n",
      "              gaming       0.98      1.00      0.99       102\n",
      "health and lifestyle       0.75      0.50      0.60       102\n",
      "    mom and children       0.92      0.94      0.93       103\n",
      "              sports       0.95      0.97      0.96       102\n",
      "                tech       0.78      0.77      0.78       102\n",
      "              travel       0.78      0.79      0.79       102\n",
      "\n",
      "            accuracy                           0.82      1022\n",
      "           macro avg       0.82      0.82      0.82      1022\n",
      "        weighted avg       0.82      0.82      0.82      1022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#burayÄ± yapmana gerek yok\n",
    "# Load and preprocess data (same as before)\n",
    "labels_data = pd.read_csv('train-classification.csv', header=None, names=['username', 'Category'])\n",
    "labels_data['Category'] = labels_data['Category'].str.lower()\n",
    "merged_data = df.merge(labels_data, left_on='username', right_on='username')\n",
    "\n",
    "X = merged_data.drop(columns=['username', 'Category'])\n",
    "y = merged_data['Category']\n",
    "\n",
    "# Convert non-numeric features to numeric\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Balance classes using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "# Scale features for better optimization\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Train SGDClassifier with class weights\n",
    "clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-3, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
    "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy_score(y_test_decoded, y_pred_decoded))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_decoded, y_pred_decoded))\n",
    "\n",
    "# In other rounds, maybe try implementing the posts texts in the df for this and see if it improves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e053cfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serha\\AppData\\Local\\Temp\\ipykernel_3992\\2227857441.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_posts[f'{i}_monthPost'] = df_posts[timestamp_col].dt.month\n",
      "C:\\Users\\serha\\AppData\\Local\\Temp\\ipykernel_3992\\2227857441.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_posts[f'{i}_yearPost'] = df_posts[timestamp_col].dt.year\n",
      "C:\\Users\\serha\\AppData\\Local\\Temp\\ipykernel_3992\\2227857441.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_posts[f'{i}_monthPost'] = df_posts[timestamp_col].dt.month\n",
      "C:\\Users\\serha\\AppData\\Local\\Temp\\ipykernel_3992\\2227857441.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_posts[f'{i}_yearPost'] = df_posts[timestamp_col].dt.year\n",
      "C:\\Users\\serha\\AppData\\Local\\Temp\\ipykernel_3992\\2227857441.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_posts[f'{i}_monthPost'] = df_posts[timestamp_col].dt.month\n",
      "C:\\Users\\serha\\AppData\\Local\\Temp\\ipykernel_3992\\2227857441.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_posts[f'{i}_yearPost'] = df_posts[timestamp_col].dt.year\n",
      "C:\\Users\\serha\\AppData\\Local\\Temp\\ipykernel_3992\\2227857441.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_posts[f'{i}_monthPost'] = df_posts[timestamp_col].dt.month\n"
     ]
    }
   ],
   "source": [
    "#burdan baÅŸlÄ±yor\n",
    "df_posts = pd.json_normalize(dataframe['posts'])\n",
    "\n",
    "def unpack_nested_column(df, column_name):\n",
    "    unpacked_df = pd.json_normalize(df[column_name])\n",
    "    unpacked_df.columns = [f\"{column_name}_{subcol}\" for subcol in unpacked_df.columns]\n",
    "    return unpacked_df\n",
    "\n",
    "unpacked_dfs = []\n",
    "for col in df_posts.columns:\n",
    "    if isinstance(df_posts[col].iloc[0], dict):\n",
    "        unpacked_df = unpack_nested_column(df_posts, col)\n",
    "        df_posts = df_posts.drop(columns=[col]).join(unpacked_df)\n",
    "    else:\n",
    "        unpacked_dfs.append(df_posts[[col]])\n",
    "\n",
    "if unpacked_dfs:\n",
    "    non_nested_df = pd.concat(unpacked_dfs, axis=1)\n",
    "    result_df = pd.concat([non_nested_df, df_posts], axis=1)\n",
    "else:\n",
    "    result_df = df_posts\n",
    "\n",
    "df_posts = result_df\n",
    "df_posts = df_posts.drop(columns=[col for col in df_posts.columns if 'media_url' in col])\n",
    "for i in range(35):\n",
    "    timestamp_col = f'{i}_timestamp'\n",
    "    if timestamp_col in df_posts.columns:\n",
    "        df_posts[timestamp_col] = pd.to_datetime(df_posts[timestamp_col], format='%Y-%m-%d %H:%M:%S')\n",
    "        df_posts[f'{i}_yearPost'] = df_posts[timestamp_col].dt.year\n",
    "        df_posts[f'{i}_monthPost'] = df_posts[timestamp_col].dt.month\n",
    "        df_posts = df_posts.drop(columns=[timestamp_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9cf19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(35):\n",
    "    timestamp_col = f'{i}_timestamp'\n",
    "    year_col = f'{i}_yearPost'\n",
    "    month_col = f'{i}_monthPost'\n",
    "\n",
    "    if timestamp_col in df_posts.columns:\n",
    "        df_posts[timestamp_col] = pd.to_datetime(df_posts[timestamp_col], format='%Y-%m-%d %H:%M:%S')\n",
    "        df_posts[year_col] = df_posts[timestamp_col].dt.year\n",
    "        df_posts[month_col] = df_posts[timestamp_col].dt.month\n",
    "        df_posts = df_posts.drop(columns=[timestamp_col])\n",
    "\n",
    "new_column_order = []\n",
    "\n",
    "for i in range(35):\n",
    "    caption_col = f'{i}_caption'\n",
    "    id_col = f'{i}_id'\n",
    "    comments_count_col = f'{i}_comments_count'\n",
    "    like_count_col = f'{i}_like_count'\n",
    "    media_type_col = f'{i}_media_type'\n",
    "    year_col = f'{i}_yearPost'\n",
    "    month_col = f'{i}_monthPost'\n",
    "    \n",
    "    if caption_col in df_posts.columns:\n",
    "        new_column_order.append(caption_col)\n",
    "    if id_col in df_posts.columns:\n",
    "        new_column_order.append(id_col)\n",
    "    if comments_count_col in df_posts.columns:\n",
    "        new_column_order.append(comments_count_col)\n",
    "    if like_count_col in df_posts.columns:\n",
    "        new_column_order.append(like_count_col)\n",
    "    if media_type_col in df_posts.columns:\n",
    "        new_column_order.append(media_type_col)\n",
    "    if year_col in df_posts.columns:\n",
    "        new_column_order.append(year_col)\n",
    "    if month_col in df_posts.columns:\n",
    "        new_column_order.append(month_col)\n",
    "\n",
    "remaining_columns = [col for col in df_posts.columns if col not in new_column_order]\n",
    "new_column_order.extend(remaining_columns)\n",
    "\n",
    "df_posts = df_posts[new_column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f6f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\serha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Error: empty vocabulary; perhaps the documents only contain stop words - Skipping TF-IDF for this DataFrame\n",
      "TF-IDF Error: empty vocabulary; perhaps the documents only contain stop words - Skipping TF-IDF for this DataFrame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text: str):\n",
    "    text = text.casefold()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Ensure nltk stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')\n",
    "\n",
    "# Directory to save TF-IDF vectorizers\n",
    "tfidf_directory = 'tfidf_vectorizers'\n",
    "os.makedirs(tfidf_directory, exist_ok=True)\n",
    "\n",
    "# Function to transform media_types into separate columns and preprocess captions\n",
    "def transform_and_preprocess(df, username):\n",
    "    # Convert media_types to separate columns\n",
    "    df['media_video'] = df['media_type'] == 'VIDEO'\n",
    "    df['media_image'] = df['media_type'] == 'IMAGE'\n",
    "    df['media_album'] = df['media_type'] == 'CAROUSEL_ALBUM'\n",
    "    df = df.drop(columns=['media_type'])\n",
    "    \n",
    "    # Preprocess caption text and filter out empty captions\n",
    "    df['caption'] = df['caption'].apply(preprocess_text)\n",
    "    df = df[df['caption'].str.strip() != '']\n",
    "    \n",
    "    # Check if there are any non-empty captions left after preprocessing\n",
    "    if df['caption'].empty or all(df['caption'].apply(lambda x: len(x.split())) == 0):\n",
    "        return df.drop(columns=['caption'])\n",
    "    \n",
    "    try:\n",
    "        # Apply TF-IDF to the non-empty captions\n",
    "        tfidf = TfidfVectorizer(max_features=150, stop_words=turkish_stopwords)\n",
    "        tfidf_matrix = tfidf.fit_transform(df['caption'])\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[word for word in tfidf.get_feature_names_out()])\n",
    "        \n",
    "        # Save the TF-IDF vectorizer to a file\n",
    "        with open(os.path.join(tfidf_directory, f'{username}_tfidf_vectorizer.pkl'), 'wb') as file:\n",
    "            pickle.dump(tfidf, file)\n",
    "        \n",
    "        # Combine TF-IDF features with the original DataFrame\n",
    "        df = df.reset_index(drop=True)\n",
    "        tfidf_df = tfidf_df.reset_index(drop=True)\n",
    "        df = pd.concat([df, tfidf_df], axis=1)\n",
    "        df = df.drop(columns=['caption'])\n",
    "    except ValueError as e:\n",
    "        print(f\"TF-IDF Error: {e} - Skipping TF-IDF for this DataFrame\")\n",
    "        df = df.drop(columns=['caption'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to create user post dataframes\n",
    "def create_user_post_dataframes(df, df_posts):\n",
    "    user_dataframes = {}  # Dictionary to store dataframes for each username\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        username = row['username']  # Get the username for this row\n",
    "        \n",
    "        # Extract all columns related to this user from df_posts\n",
    "        user_post_data = {\n",
    "            'caption': [],\n",
    "            'id': [],\n",
    "            'comments_count': [],\n",
    "            'like_count': [],\n",
    "            'media_type': [],\n",
    "            'yearPost': [],  # Changed to yearPost\n",
    "            'monthPost': []  # Changed to monthPost\n",
    "        }\n",
    "        \n",
    "        for i in range(34):  # Assuming up to 34 posts\n",
    "            caption_col = f'{i}_caption'\n",
    "            id_col = f'{i}_id'\n",
    "            comments_count_col = f'{i}_comments_count'\n",
    "            like_count_col = f'{i}_like_count'\n",
    "            media_type_col = f'{i}_media_type'\n",
    "            yearPost_col = f'{i}_yearPost'\n",
    "            monthPost_col = f'{i}_monthPost'\n",
    "            \n",
    "            # Ensure all required columns exist in df_posts\n",
    "            if caption_col in df_posts.columns and id_col in df_posts.columns and comments_count_col in df_posts.columns and like_count_col in df_posts.columns and media_type_col in df_posts.columns and yearPost_col in df_posts.columns and monthPost_col in df_posts.columns:\n",
    "                # Add the values for this post\n",
    "                caption = df_posts.loc[index, caption_col]\n",
    "                post_id = df_posts.loc[index, id_col]\n",
    "                comments_count = df_posts.loc[index, comments_count_col]\n",
    "                like_count = df_posts.loc[index, like_count_col]\n",
    "                media_type = df_posts.loc[index, media_type_col]\n",
    "                yearPost = df_posts.loc[index, yearPost_col]\n",
    "                monthPost = df_posts.loc[index, monthPost_col]\n",
    "                \n",
    "                # Check if the row is not empty\n",
    "                if pd.notna(caption) and pd.notna(post_id) and pd.notna(comments_count) and pd.notna(like_count) and pd.notna(media_type) and pd.notna(yearPost) and pd.notna(monthPost):\n",
    "                    user_post_data['caption'].append(caption)\n",
    "                    user_post_data['id'].append(post_id)\n",
    "                    user_post_data['comments_count'].append(comments_count)\n",
    "                    user_post_data['like_count'].append(like_count)\n",
    "                    user_post_data['media_type'].append(media_type)\n",
    "                    user_post_data['yearPost'].append(yearPost)  # Append to yearPost\n",
    "                    user_post_data['monthPost'].append(monthPost)  # Append to monthPost\n",
    "\n",
    "        # Create a dataframe for this user with the extracted data\n",
    "        user_df = pd.DataFrame(user_post_data)\n",
    "        \n",
    "        # Transform media_types and preprocess captions\n",
    "        if not user_df.empty:\n",
    "            user_df = transform_and_preprocess(user_df, username)\n",
    "        \n",
    "        # Save it to the dictionary if it's not empty\n",
    "        if not user_df.empty:\n",
    "            user_dataframes[username] = user_df\n",
    "    \n",
    "    return user_dataframes\n",
    "\n",
    "# Usage\n",
    "user_dataframes = create_user_post_dataframes(df, df_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acdb9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_json('test-regression-round3.jsonl', lines=True)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df['timestamp'])\n",
    "input_df['yearPost'] = input_df['timestamp'].dt.year\n",
    "input_df['monthPost'] = input_df['timestamp'].dt.month\n",
    "input_df = input_df.drop(columns=['timestamp'])\n",
    "input_df = input_df.drop(columns=['media_url'])\n",
    "input_df['media_video'] = input_df['media_type'] == 'VIDEO'\n",
    "input_df['media_image'] = input_df['media_type'] == 'IMAGE'\n",
    "input_df['media_album'] = input_df['media_type'] == 'CAROUSEL_ALBUM'\n",
    "input_df = input_df.drop(columns=['media_type'])\n",
    "input_df['like_count'] = float('nan')  \n",
    "input_df = input_df[['id', 'comments_count', 'like_count', 'yearPost', 'monthPost', 'media_video', 'media_image', 'media_album', 'username', 'caption']]\n",
    "input_df['caption'] = input_df.apply(lambda row: row['username'] if pd.isna(row['caption']) else row['caption'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62c4bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been written to prediction-regression-round3.jsonl\n"
     ]
    }
   ],
   "source": [
    "#final hali\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load test data with exact ID handling\n",
    "test_df = pd.read_json('test-regression-round3.jsonl', lines=True, dtype={'id': str})\n",
    "\n",
    "# Process timestamp for test data\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "test_df['yearPost'] = test_df['timestamp'].dt.year\n",
    "test_df['monthPost'] = test_df['timestamp'].dt.month\n",
    "test_df = test_df.drop(columns=['timestamp'])\n",
    "test_df = test_df.drop(columns=['media_url'])\n",
    "\n",
    "# Process media types for test data\n",
    "test_df['media_video'] = test_df['media_type'] == 'VIDEO'\n",
    "test_df['media_image'] = test_df['media_type'] == 'IMAGE'\n",
    "test_df['media_album'] = test_df['media_type'] == 'CAROUSEL_ALBUM'\n",
    "test_df = test_df.drop(columns=['media_type'])\n",
    "\n",
    "# Add NaN like_count column to test data\n",
    "test_df['like_count'] = float('nan')\n",
    "\n",
    "# Reorder columns to match training data\n",
    "test_df = test_df[['id', 'comments_count', 'like_count', 'yearPost', 'monthPost', \n",
    "                   'media_video', 'media_image', 'media_album', 'username', 'caption']]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Create list to store predictions in order\n",
    "predictions = []\n",
    "\n",
    "# Process only test data rows\n",
    "for i in range(len(test_df)):\n",
    "    try:\n",
    "        # Get the test row\n",
    "        test_row = test_df.iloc[i]\n",
    "        username = test_row['username']\n",
    "        row_id = str(test_row['id'])  # Convert to string for exact matching\n",
    "        caption = test_row['caption']\n",
    "\n",
    "        tfidf_file_path = os.path.join(tfidf_directory, f'{username}_tfidf_vectorizer.pkl')\n",
    "\n",
    "        # Skip if username is missing or TF-IDF file doesn't exist\n",
    "        if not username or not os.path.exists(tfidf_file_path):\n",
    "            predictions.append((row_id, 0))\n",
    "            continue\n",
    "\n",
    "        # Get only training data for this user\n",
    "        training_df = user_dataframes.get(username)\n",
    "        \n",
    "        # Skip if no training data exists\n",
    "        if training_df is None or training_df.empty:\n",
    "            predictions.append((row_id, 0))\n",
    "            continue\n",
    "\n",
    "        # Skip if the training data doesn't have like_count values\n",
    "        if 'like_count' not in training_df.columns or training_df['like_count'].isna().all():\n",
    "            predictions.append((row_id, 0))\n",
    "            continue\n",
    "\n",
    "        # Only use rows with valid like_count for training\n",
    "        training_df = training_df.dropna(subset=['like_count'])\n",
    "\n",
    "        # Skip if insufficient training data\n",
    "        if len(training_df) < 2:\n",
    "            predictions.append((row_id, 0))\n",
    "            continue\n",
    "\n",
    "        # Load and apply TF-IDF transformation\n",
    "        with open(tfidf_file_path, 'rb') as file:\n",
    "            tfidf_vectorizer = pickle.load(file)\n",
    "\n",
    "        # Transform test row\n",
    "        if (caption):\n",
    "            preprocessed_caption = preprocess_text(caption)\n",
    "        \n",
    "        \n",
    "        tfidf_vector = tfidf_vectorizer.transform([preprocessed_caption])\n",
    "        tfidf_df = pd.DataFrame(tfidf_vector.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # Prepare test features\n",
    "        transformed_row = pd.concat([\n",
    "            test_row.drop(labels=['caption']).to_frame().T.reset_index(drop=True),\n",
    "            tfidf_df.reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Ensure column alignment with training data\n",
    "        for col in training_df.columns:\n",
    "            if col not in transformed_row.columns:\n",
    "                transformed_row[col] = 0\n",
    "\n",
    "        # Prepare features\n",
    "        exclude_cols = {'like_count', 'id', 'username'}\n",
    "        features = [col for col in training_df.columns if col not in exclude_cols]\n",
    "\n",
    "        # Prepare training data\n",
    "        train_features = training_df[features].fillna(0)\n",
    "        train_target = training_df['like_count']\n",
    "\n",
    "        # Prepare test features\n",
    "        test_features = transformed_row[features].fillna(0)\n",
    "\n",
    "        # Ensure all features are numeric\n",
    "        train_features = train_features.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        test_features = test_features.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "        # Align columns\n",
    "        common_cols = list(set(train_features.columns) & set(test_features.columns))\n",
    "        train_features = train_features[common_cols]\n",
    "        test_features = test_features[common_cols]\n",
    "\n",
    "        # Train and predict\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        model.fit(train_features, train_target)\n",
    "        predicted_like_count = int(round(max(0, model.predict(test_features)[0])))\n",
    "\n",
    "        predictions.append((row_id, predicted_like_count))\n",
    "\n",
    "    except Exception as e:\n",
    "        predictions.append((row_id, 0))\n",
    "        if \"Input contains NaN\" not in str(e):\n",
    "            print(f\"Error for row {row_id}: {str(e)}\")\n",
    "\n",
    "# Write predictions to JSONL file maintaining original order\n",
    "with open('prediction-regression-round3.jsonl', 'w') as f:\n",
    "    for i, (id_, like_count) in enumerate(predictions):\n",
    "        line = f'    \"{id_}\": {like_count}'\n",
    "        if i < len(predictions) - 1:\n",
    "            line += ','\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(\"Predictions have been written to prediction-regression-round3.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1L432loBfB0HmjSF49ekpfIeX7tqntUCx",
     "timestamp": 1729864857556
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
